h3. Overview

Please test the new ec2-management v2 script. This program provides a set of commands via a command-line interface (CLI) that perform operations on a Matterhorn ec2 cluster. This is a substantial overhaul of the v1 code and all functionality needs to be reviewed.

The new code is deployed to the "Instance-manager2" ec2 instance, ip 54.85.72.8. To QA you'll need to ssh to that instance as the 'ansible' user and execute a range of command-line operations, then check the resulting state of the cluster being operated on.

I'm not sure what cluster(s) you'll have access to for testing, but to fully test all functionality it needs to be one with an associated Zadara array. Failing that, dev04 is available and has enough workers (4) to test everything else.

h3. Setup

There are a few initial steps that must done prior to actually executing the script commands being tested. You'll need to do these each time you begin QA.

1. ssh to the instance: {{ssh ansible@54.85.72.8}}
2. change to the right directory: {{cd ~/ec2-management-v2/QA}}
3. activate the python environment: {{source ../venv/bin/activate}}
4. unset the default aws credentials: {{unset AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID}}

h3. Assumed settings

You should hopefully not need to modify settings.py. The following settings values are assumed unless otherwise stated below:

* MAX_WORKERS=3
* MIN_WORKERS=2
* MIN_IDLE_WORKERS=1
* MAX_QUEUED_JOBS=0

h3. _dry-run_ enabled

All of the following commands to test should be no-ops if the _dry-run_ option is included, meaning the operation should complete with no actual changes made. To confirm this it might make the most sense to execute each one first with _--dry-run_ and then again for the actual QA test. 

To enable _dry-run_ insert "--dry-run" prior to the cluster prefix, e.g.: {{./ec2_manager.py --dry-run cluster stop}}

h3. cluster prefix

Each command must include the prefix (name) of the cluster. I've gone ahead and assumed {{dev04}} for all of the following examples. If you end up testing on a different cluster you'll need to modify the command.

h3. Commands to test:

h4. cluster status

*Command*: {{./ec2_manager.py dev04 status}}

*Expected result*: a json dump of the cluster status summary should output to the terminal. If the cluster is offline It should include the following values:

* cluster prefix
* number of instances
* number of instances online
* number of workers
* number of workers online
* a list of instances, each with the following values:
** instance id
** instance name
** instance state

If the cluster is running the following additional values will be included:

* number of running jobs
* number of queued jobs
* number of "high load" jobs
* each item in the list of instances will also include the node's maintenance state

h4. cluster start

h6. vanilla start

*Preconditions*: all ec2 instances in state "stopped"

*Command*: {{./ec2_manager.py dev04 start}}

*Expected result*: cluster's ec2 instances in "running" state, including admin, engage, db, nfs and default number of workers. Default is whatever settings.MIN_WORKERS is (should be 2).

h6. start with specific number of workers

*Preconditions*: all ec2 instances in state "stopped"

*Command*: {{./ec2_manager.py dev04 start -w 3}}

*Expected result*: all non-worker instances + 3 workers in "running" state

h6. start with too many workers requested

*Preconditions*: all ec2 instances in state "stopped"

*Command*: {{./ec2_manager.py dev04 start -w 5}}

*Expected result*: Process should fail, complaining about not enough workers available.

h4. cluster stop

h6. vanilla stop

*Preconditions*: admin, support instances + _n_ workers in "running" state

*Command*: {{./ec2_manager.py dev04 stop}}

*Expected result*: all instances in "stopped" state

h6. stop w/ held instance

*Preconditions*: admin, support + _n_ workers running. Admin instance ec2 tags include "hold=1"

*Expected result*: Process should fail, complaining about held instances

h6. stop w/ held instance and --force

*Preconditions*: same as previous

*Command*: {{./ec2_manager.py --force dev04 stop}}

*Expected result*: Warning about held instance, but process should continue. All instances in "stopped" state.

h6. stop with non-idle workers

*Preconditions*: admin, support + _n_ workers running. At least one worker processing jobs.

*Command*: {{./ec2_manager.py dev04 stop}}

*Expected result*: process should fail after waiting a bit for workers to be idle

h6. stop with non-idle workers and --force

*Preconditions*: admin, support + _n_ workers running. At least one worker processing jobs.

*Command*: {{./ec2_manager.py --force dev04 stop}}

*Expected result*: warning about retries exceeded, but process should continue. All instances in "stopped" state.

h4. cluster autoscale

h6. Notes

For the "busy worker" tests it may be difficult to asses whether any jobs are actually queued since the Matterhorn statistics interface does not correctly report them. I think the best method is to use the {{status}} command which will output information on "queued_jobs" and "queued_high_load_jobs". It's the "queued_high_load_jobs" value that will trigger the autoscaling in the "up" direction.

{{./ec2_manager.py dev04 status}}

When testing autoscale there is an additional aspect that comes into play when shutting down idle workers. When an ec2 instance is started it gets billed for a full hour of uptime, even if it is subsequently shut down 5 minutes later. Therefore, once the process has confirmed that there are idle workers, it then looks at how long the instances have been "up" and rejects those whose uptime calculations indicate the instance has not used the bulk of it's billed hour. The threshold of what constitutes "the bulk of it's billed hour" is defined by {{settings.IDLE_INSTANCE_UPTIME_THRESHOLD}} (currently 50 mintues). For example, if an instance is seen to have only been up for 40 minutes (or 1:40, 3:33, etc.) it will not be shut down even if idle. If it has been up for 53 minutes (or 1:53, 5:53, etc.) it will be shut down.

It's easy to manipulate the clock within unittests to test this logic, but in manual QA tests it means in some cases you will need to perform some precondition setup, then wait for _x_ minutes of instance uptime before running the command and confirming the expected behavior. Alternatively, you could tweak the {{settings.IDLE_INSTANCE_UPTIME_THRESHOLD}} value to something like 5 if you're comfortable doing so.

This billing-avoidance strategy only comes into play when scaling down; the autoscale tests for going in the other direction you don't need to
Finally, it might be helpful to run these commands with the {{--debug}} flag to get the extra output/info about what's going on under the hood.

h6. idle workers, recently started

*Preconditions*: admin, support + 4 workers running. Workers idle. Instance uptime < {{settings.IDLE_INSTANCE_UPTIME_THRESHOLD}}

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: No changes. Output complains no workers available to stop.

h6. idle workers, not-so-recently started

*Preconditions*: admin, support + 4 workers running. Workers idle. Instance uptime > {{settings.IDLE_INSTANCE_UPTIME_THRESHOLD}}

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: 1 worker stopped.

h6. idle workers, not-so-recently started, pt 2

*Preconditions*: admin, support + 3 workers running. Workers idle. Instance uptime > {{settings.IDLE_INSTANCE_UPTIME_THRESHOLD}}

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: 1 worker stopped

h6. idle workers, but MIN_WORKERS reached

The MIN_WORKERS check happens before the uptime check, so instance uptime shouldn't matter here.

*Preconditions*: admin, support + 2 workers running. Workers idle.

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: warning that MIN_WORKERS setting prevented stopping a worker. No changes.

h6. busy workers

*Preconditions*: admin, support + 2 workers running. Workers busy so that some number of jobs of types _composer_, _inspection_, _videoeditor_, or _videosegmenter_ are queued.

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: 1 worker should be started

h6. busy workers but MAX_WORKERS reached

*Preconditions*: admin, support + 3 workers running. Workers busy so that some number of jobs of types _composer_, _inspection_, _videoeditor_, or _videosegmenter_ are queued. 

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: warning that MAX_WORKERS setting prevented starting a worker. No changes.

h6. some busy workers but MIN_IDLE_WORKERS reached

*Preconditions*: admin, support + 3 workers running. 2 workers busy, 1 worker idle, no jobs queued.

*Command*: {{./ec2_manager.py dev04 autoscale}}

*Expected result*: No changes.

h4. cluster scale up

h6. vanilla scale up

*Preconditions*: admin, support + 1 worker running.

*Command*: {{./ec2_manager.py dev04 scale up}}

*Expected result*:  1 worker started

h6. scale up _n_ workers

*Preconditions*: admin, support + 1 workers running.

*Command*: {{./ec2_manager.py dev04 scale -w 2 up}}

*Expected result*:  2 workers started

h6. scale up > MAX_WORKERS

*Preconditions*: admin, support + 2 workers running.

*Command*: {{./ec2_manager.py dev04 scale -w 2 up}}

*Expected result*:  process fails complaining about MAX_WORKERS

h4. cluster scale down

h6. vanilla scale down

*Preconditions*: admin, support + 3 workers running.

*Command*: {{./ec2_manager.py dev04 scale down}}

*Expected result*: 1 worker stopped.

h6. scale down _n_ workers

*Preconditions*: admin, support + 4 workers running.

*Command*: {{./ec2_manager.py dev04 scale -w 2 down}}

*Expected result*: 2 workers stopped.

h6. scale down < MIN_WORKERS

*Preconditions*: admin, support + 3 workers running.

*Command*: {{./ec2_manager.py dev04 scale -w 2 down}}

*Expected result*: process fails complaining about MIN_WORKERS

h6. scale down < MIN_WORKERS with --force

*Preconditions*: admin, support + 3 workers running.

*Command*: {{./ec2_manager.py --force dev04 scale -w 2 down}}

*Expected result*: process warns about MIN_WORKERS but proceeds. 2 workers stopped.

h6. scale down, no idle workers

*Preconditions*: admin, support + 3 workers running. All workers busy with running jobs.

*Command*: {{./ec2_manager.py dev04 scale -w 2 down}}

*Expected result*: process fails complaining about no idle workers.

h6. scale down, no idle workers + --force

*Preconditions*: admin, support + 3 workers running. All workers busy with running jobs.

*Command*: {{./ec2_manager.py --force dev04 scale -w 2 down}}

*Expected result*: process warns about no idle workers but shuts down 2 anyway.

h4. cluster maintenance on

*Preconditions*: admin, support + _n_ workers running. Some or all instances not in maintenance.

*Command*: {{./ec2_manager.py dev04 maintenance on}}

*Expected result*: All nodes in maintenance mode.

h4. cluster maintenance off

*Preconditions*: admin, support + _n_ workers running. Some or all instances in maintenance.

*Command*: {{./ec2_manager.py dev04 maintenance off}}

*Expected result*: All nodes taken out of maintenance mode.

h4. cluster scale_to

h6. scale_to _n_ workers

*Preconditions*: admin, support + 1 workers running (use AWS console to shut off all but 1 worker)

*Command*: {{./ec2_manager.py dev04 scale_to 3}}

*Expected result*: 2 workers are started

h6. scale_to _n_ workers, pt 2

*Preconditions*: admin, support + 4 workers running (use AWS console to start all workers)

*Command*: {{./ec2_manager.py dev04 scale_to 2}}

*Expected result*: 2 workers are stopped

h6. scale_to > MAX_WORKERS

*Preconditions*: admin, support + 2 workers running

*Command*: {{./ec2_manager.py dev04 scale_to 4}}

*Expected result*: process fails complaning about exceeding MAX_WORKERS

h6. scale_to < MIN_WORKERS

*Preconditions*: admin, support + 3 workers running

*Command*: {{./ec2_manager.py dev04 scale_to 1}}

*Expected result*: process fails complaning about exceeding MIN_WORKERS
